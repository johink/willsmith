{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_MOVES = 81 # Max number of valid moves in a game of NestedTTT\n",
    "\n",
    "class NestedTTTNet(nn.Module):\n",
    "    def __init__(self, inner_board_units = 32, outer_board_units = 64, final_units = 256, n_res_layers = 2):\n",
    "        super(NestedTTTNet, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.outer_board_units = outer_board_units\n",
    "        self.final_units = final_units\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, inner_board_units, 3, padding = 1)\n",
    "        self.bn_inner = nn.BatchNorm2d(inner_board_units)\n",
    "        self.res1 = nn.Conv2d(inner_board_units, inner_board_units, 3, padding = 1)\n",
    "        \n",
    "        self.conv2_collapse = nn.Conv2d(inner_board_units, outer_board_units, 3)\n",
    "        self.bn_outer = nn.BatchNorm2d(outer_board_units)\n",
    "        self.res2 = nn.Conv2d(outer_board_units, outer_board_units, 3, padding = 1)\n",
    "        \n",
    "        self.conv3_collapse = nn.Conv2d(outer_board_units, final_units, 3)\n",
    "        self.bn_linear = nn.BatchNorm1d(final_units)\n",
    "        \n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(final_units, NUM_MOVES),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(final_units, final_units // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_units // 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, board_states):\n",
    "        '''\n",
    "        board_states.size() == (-1, 3, 3, 3, 3, 3)\n",
    "                            == (batch_size, [x-owned, o-owned, turn-state], outer_row, outer_col, inner_row, inner_col)\n",
    "        '''\n",
    "        # Move the outer board states to the left so their intra-square can be resolved before introducing inter-square\n",
    "        board_states = board_states.permute(0, 2, 3, 1, 4, 5).contiguous().view(-1, 3, 3, 3)\n",
    "        \n",
    "        x = F.relu(self.bn_inner(self.conv1(board_states)))\n",
    "        for _ in range(self.n_res_layers):\n",
    "            x_conv = F.relu(self.bn_inner(self.res1(x)))\n",
    "            x = F.relu(x + self.bn_inner(self.res1(x_conv)))\n",
    "            \n",
    "        x = F.relu(self.bn_outer(self.conv2_collapse(x)))\n",
    "        x = x.view(-1, 3, 3, self.outer_board_units).permute(0, 3, 1, 2).contiguous() # Move them back to the right\n",
    "        \n",
    "        for _ in range(self.n_res_layers):\n",
    "            x_conv = F.relu(self.bn_outer(self.res2(x)))\n",
    "            x = F.relu(x + self.bn_outer(self.res2(x_conv)))\n",
    "        \n",
    "        x = F.relu(self.bn_linear(self.conv3_collapse(x).view(-1, self.final_units)))\n",
    "        \n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        \n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NestedTTT:\n",
    "\n",
    "    anti_diag_tensor = torch.ByteTensor([[1 if r + c == 2 else 0 for c in range(3)] for r in range(3)])\n",
    "\n",
    "    def __init__(self):\n",
    "        #Board State Tensor\n",
    "        #Outermost dimension:\n",
    "        #First value represents Player 0's (X's) holdings, second represents O's\n",
    "        #Third is filled with the number of the current player to move (0 = X, 1 = O)\n",
    "        #Rest of the dimensions are (Outer Row, Outer Column, Inner Row, Inner Column)\n",
    "        self.state = torch.full((3, 3, 3, 3, 3), 0, dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "        #Valid Move Tensor\n",
    "        #(Outer Row, Outer Column, Inner Row, Inner Column)\n",
    "        self.valid_moves = torch.full((3, 3, 3, 3), 1, dtype = torch.uint8, requires_grad = False)\n",
    "\n",
    "        #Summary Board Tensor\n",
    "        #(Player, Row, Column)\n",
    "        self.summary_boards = torch.full((2, 3, 3), 0, dtype = torch.uint8, requires_grad = False)\n",
    "\n",
    "        #Current Player\n",
    "        self.current_player = 0\n",
    "        \n",
    "    def check_win(self, board):\n",
    "        \"\"\"\n",
    "        Short-circuits if current player has less than 3 squares on the current board\n",
    "        If they do, checks all rows, columns, and diagonals for a win\n",
    "\n",
    "        Returns 0 if the current player did not win the board in question, or 1 if they did\n",
    "        \"\"\"\n",
    "        if board.sum().item() < 3:\n",
    "            return 0\n",
    "        else:\n",
    "            return (board.sum(0) == 3).any().item() or \\\n",
    "                   (board.sum(1) == 3).any().item() or \\\n",
    "                   (board.diag().sum() == 3).item()   or \\\n",
    "                   (board.masked_select(self.anti_diag_tensor).sum() == 3).item()\n",
    "\n",
    "    def make_move(self, move):\n",
    "        \"\"\"\n",
    "        Takes a move:\n",
    "          ({0 for X, 1 for O}, outer row, outer col, inner row, inner col)\n",
    "        Returns 1 if the move won the game, 0 otherwise\n",
    "        \"\"\"\n",
    "        if not self.valid_moves[move[1:]].item():\n",
    "            raise ValueError(\"Invalid move placement\")\n",
    "        if move[0] != self.state[2,0,0,0,0].item():\n",
    "            raise ValueError(\"Only the current player can make a move\")\n",
    "        self.state[move] = 1\n",
    "        self.valid_moves[move[1:]] = 0\n",
    "        self.state[2] = 1 - self.state[2]\n",
    "\n",
    "        if self.check_win(self.state[move[:3]]):\n",
    "            self.summary_boards[move[:3]] = 1\n",
    "            self.valid_moves[move[1:3]] = 0\n",
    "            if self.check_win(self.summary_boards[move[0]]):\n",
    "                self.valid_moves.fill_(0)\n",
    "                return 1\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def undo_move(self, move):\n",
    "        if not self.state[move].item():\n",
    "            raise ValueError(\"Can only undo where a move has been played\")\n",
    "        if move[0] == self.current_player:\n",
    "            raise ValueError(\"Cannot undo a move for the current player\")\n",
    "        if self.state[(2,0,0,0,0)].item() != self.current_player:\n",
    "            raise ValueError(\"Desynchronization between current player and state tensor\")\n",
    "        self.state[move] = 0\n",
    "        self.valid_moves[move[1:3]] = 1 - self.state[:2, move[1], move[2]].sum(0)\n",
    "\n",
    "        self.summary_boards[move[:3]] = 0\n",
    "        self.state[2] = 1 - self.state[2]\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = (self.current_player + 1) % 2\n",
    "        \n",
    "    def copy(self):\n",
    "        c = self.__new__(NestedTTT)\n",
    "        c.state = self.state.clone().detach().detach()\n",
    "        c.valid_moves = self.valid_moves.clone().detach()\n",
    "        c.summary_boards = self.summary_boards.clone().detach()\n",
    "        c.current_player = self.current_player\n",
    "        return c\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        \n",
    "    def get_valid_moves(self):\n",
    "        return self.valid_moves.nonzero().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "EXPLORATION_PARAM = sqrt(2)\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Nodes represent a state in the game.  They can have edges which\n",
    "    indicate legal actions to take from the state\n",
    "    \"\"\"\n",
    "    def __init__(self, incoming_edge):\n",
    "        self.parent_edge = incoming_edge\n",
    "        self.leaf = True\n",
    "        self.edges = dict()\n",
    "\n",
    "    def choose_action(self, temperature = 1):\n",
    "        if not self.edges:\n",
    "            raise ValueError(\"Cannot choose action from an unexplored node\")\n",
    "\n",
    "        visits_exp = sum([edge.N ** (1 / temperature) for edge in self.edges.values()])\n",
    "        thresh = random.random()\n",
    "        cumsum = 0\n",
    "        for action, edge in self.edges.items():\n",
    "            cumsum = cumsum + edge.N ** (1 / temperature)\n",
    "            if thresh < cumsum / visits_exp:\n",
    "                return action\n",
    "\n",
    "    def selection_step(self):\n",
    "        if self.leaf:\n",
    "            raise ValueError(\"Cannot perform selection step at a leaf\")\n",
    "        max_result = None\n",
    "        max_action = None\n",
    "        \n",
    "        total_trials = sum([edge.N for edge in self.edges.values()])\n",
    "\n",
    "        for action, edge in self.edges.items():\n",
    "            current_estimate = edge.value_estimate(total_trials)\n",
    "            if max_result is None or max_result < current_estimate:\n",
    "                max_result = current_estimate\n",
    "                max_action = action\n",
    "\n",
    "        return max_action\n",
    "    \n",
    "    def expansion(self, actions, ps):\n",
    "        if actions:\n",
    "            for action, p in zip(actions, ps):\n",
    "                self.edges[action] = Edge(self, p)\n",
    "            self.leaf = False\n",
    "        \n",
    "    def backpropagate(self, value):\n",
    "        if self.parent_edge is None:\n",
    "            raise ValueError(\"Can't start backpropagating at the root\")\n",
    "        else:\n",
    "            #Current state for player A is a result of the action (edge) chosen by player B\n",
    "            #If the current state has a good value for player A, then it has a bad value for player B\n",
    "            #Value of current state (node) is inverted to update value of action taken (incoming edge)\n",
    "            self.parent_edge.backpropagate(-value)\n",
    "\n",
    "class Edge:\n",
    "    \"\"\"\n",
    "    Edges represent transitions between game states, initiated by an action\n",
    "    Edges keep track of the number of visits, the estimated value of taking\n",
    "    their associated action, and the prior probability of taking that action\n",
    "    \"\"\"\n",
    "    def __init__(self, origin, p):\n",
    "        self.origin = origin\n",
    "        self.destination = Node(self)\n",
    "        self.N = 0 # Visits\n",
    "        self.Q = 0 # Value estimate\n",
    "        self.P = p # NN-generated prior\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.N += 1\n",
    "        self.Q = self.Q * ((self.N - 1) / self.N) + value / self.N\n",
    "        \n",
    "        #Alternating edges indicate alternating player actions, so the value of states will be inverted at each layer\n",
    "        if self.origin.parent_edge is not None:\n",
    "            self.origin.parent_edge.backpropagate(-value)\n",
    "\n",
    "    def value_estimate(self, parent_trials):\n",
    "        return self.Q + EXPLORATION_PARAM * self.P * parent_trials ** .5 / (1 + self.N)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"N={}, Q={}, P={}\".format(self.N, self.Q, self.P)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "class AlphaMCTSAgent:\n",
    "    def __init__(self, control_net = None):\n",
    "        self.root = Node(None)\n",
    "        self.playout_total = 0\n",
    "        self.control_net = control_net\n",
    "        \n",
    "    def update_control_net(self, control_net):\n",
    "        self.control_net = control_net\n",
    "        \n",
    "    def search(self, game, turn_num, allotted_playouts = 800):\n",
    "        if self.control_net is None:\n",
    "            raise ValueError(\"Control net must be set before starting searches\")\n",
    "        playouts = 0\n",
    "\n",
    "        start_time = time()\n",
    "        while playouts < allotted_playouts:\n",
    "            current_game = game.copy()\n",
    "            current_game, node = self._selection(current_game)\n",
    "\n",
    "            actions = current_game.get_valid_moves()\n",
    "            ps, v = self.control_net(current_game.state.unsqueeze(0))\n",
    "            ps.squeeze_()\n",
    "            v = v.squeeze().item()\n",
    "\n",
    "            indices = [self.ttt_position_to_index(action) for action in actions]\n",
    "            \n",
    "            actions = [(round(current_game.state[2,0,0,0,0].item()),) + tuple(action) for action in actions]\n",
    "\n",
    "            node.expansion(actions, ps.detach().numpy()[indices])\n",
    "            \n",
    "            if id(self.root) != id(node):\n",
    "                node.backpropagate(v)\n",
    "\n",
    "            playouts += 1\n",
    "\n",
    "        temp = 1 if turn_num < 30 else .1\n",
    "        max_action = self.root.choose_action(temp)\n",
    "        \n",
    "        # debug info\n",
    "        self.playout_total = playouts\n",
    "        self.action_node = max_action\n",
    "        \n",
    "        actions = game.get_valid_moves()\n",
    "        indices = [self.ttt_position_to_index(action) for action in actions]\n",
    "        actions = [(round(game.state[2,0,0,0,0].item()),) + tuple(action) for action in actions]\n",
    "        \n",
    "        mcts_probs = {indices[i]: self.root.edges[actions[i]].N for i in range(len(indices))}\n",
    "        total_trials = sum(mcts_probs.values())\n",
    "        mcts_probs = [mcts_probs.get(x, 0) / total_trials for x in range(81)]\n",
    "        \n",
    "        return max_action, mcts_probs\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        try:\n",
    "            self.root = self.root.edges[action].destination\n",
    "        except KeyError:\n",
    "            print(\"Action not found, throwing away tree\")\n",
    "            self.reset()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.root = Node(None)\n",
    "\n",
    "    def _selection(self, state):\n",
    "        \"\"\"\n",
    "        Progress through the tree of Nodes, starting at the root, until a\n",
    "        leaf is found or there are unexplored actions at the level we are\n",
    "        exploring.\n",
    "\n",
    "        Uses the UCT algorithm to determine which nodes to progress to.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        while not node.leaf:\n",
    "            move = node.selection_step()\n",
    "            node = node.edges[move].destination\n",
    "            state.make_move(move)\n",
    "        return state, node\n",
    "    \n",
    "    @staticmethod\n",
    "    def index_to_ttt_position(idx):\n",
    "        return idx // 27 % 3, idx // 9 % 3, idx // 3 % 3, idx % 3\n",
    "\n",
    "    @staticmethod\n",
    "    def ttt_position_to_index(position):\n",
    "        return position[0] * 27 + position[1] * 9 + position[2] * 3 + position[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity = 100000):\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "    \n",
    "    def push(self, data):\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def extend(self, data):\n",
    "        self.buffer.extend(data)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One class that has access to:\n",
    "  # Game simulation\n",
    "  # Control network\n",
    "  # Current network\n",
    "  # Replay buffer\n",
    "  # MCTS agent\n",
    "  # Training code  -- Another process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def zero_gen():\n",
    "    while True:\n",
    "        yield 0\n",
    "\n",
    "def one_neg_one_gen():\n",
    "    while True:\n",
    "        yield 1\n",
    "        yield -1\n",
    "\n",
    "class SelfPlayTrainer:\n",
    "    def __init__(self, agent, game, buffer_file = None, weights_file = None, n_batches = 0):\n",
    "        self.agent = agent\n",
    "        self.game = game\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        if buffer_file is not None:\n",
    "            self.replay_buffer.buffer = pickle.load(open(buffer_file, \"rb\"))\n",
    "        \n",
    "        self.current_network = NestedTTTNet()\n",
    "        self.control_network = NestedTTTNet()\n",
    "        \n",
    "        if weights_file is not None:\n",
    "            self.control_network.load_state_dict(torch.load(weights_file))\n",
    "        \n",
    "        self.current_network.load_state_dict(self.control_network.state_dict())\n",
    "        self.control_network.eval()\n",
    "        self.current_network.train()\n",
    "        \n",
    "        self.agent.update_control_net(self.control_network)\n",
    "        \n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "        self.optim = torch.optim.Adam(self.current_network.parameters(), lr = .01, weight_decay = 10e-4)\n",
    "    \n",
    "    def generate_self_play_data(self, n_games = 100):\n",
    "        for _ in range(n_games):\n",
    "            turn_num = 0\n",
    "            self.game.reset()\n",
    "            self.agent.reset()\n",
    "            result = 0\n",
    "            player_num = 0\n",
    "\n",
    "            states = []\n",
    "            move_vectors = []\n",
    "\n",
    "            while len(self.game.get_valid_moves()) > 0:\n",
    "                move, move_probs = self.agent.search(self.game.copy(), turn_num, allotted_playouts = 400)\n",
    "\n",
    "                states.append(self.game.state.tolist())\n",
    "                move_vectors.append(move_probs)\n",
    "\n",
    "                result = self.game.make_move(move)\n",
    "                if not result:\n",
    "                    self.game.switch_player()\n",
    "                    self.agent.take_action(move)\n",
    "                    turn_num += 1\n",
    "                    player_num = (player_num + 1) % 2\n",
    "\n",
    "            if not result:\n",
    "                self.replay_buffer.extend(list(zip(states, move_vectors, zero_gen())))\n",
    "            else:\n",
    "                self.replay_buffer.extend(list(zip(states[::-1], move_vectors[::-1], one_neg_one_gen()))[::-1])\n",
    "\n",
    "    def compare_control_to_train(self):\n",
    "        self.current_net.eval()\n",
    "        old_agent = AlphaMCTSAgent(control_net = self.control_net)\n",
    "        new_agent = AlphaMCTSAgent(control_net = self.current_net)\n",
    "        \n",
    "        agents = [old_agent, new_agent]\n",
    "        \n",
    "        wins = 0\n",
    "        ties = 0\n",
    "        \n",
    "        game = self.game.copy()\n",
    "        \n",
    "        for game_num in range(100):\n",
    "            game.reset()\n",
    "            agents[0].reset()\n",
    "            agents[1].reset()\n",
    "            result = 0\n",
    "            player_num = game_num // 50 #Both take first turn 50 times\n",
    "            turn_num = 100 #Turn down the temperature\n",
    "            \n",
    "            while len(game.get_valid_moves()) > 0:\n",
    "                move, _ = agents[player_num].search(game.copy(), turn_num, allotted_playouts = 800)\n",
    "                _, _ = agents[1 - player_num].search(game.copy(), turn_num, allotted_playouts = 800)\n",
    "\n",
    "                result = self.game.make_move(move)\n",
    "                if not result:\n",
    "                    game.switch_player()\n",
    "                    agents[0].take_action(move)\n",
    "                    agents[1].take_action(move)\n",
    "                    player_num = (player_num + 1) % 2\n",
    "\n",
    "            if not result:\n",
    "                ties += 1\n",
    "            elif result and current_player == 1:\n",
    "                wins += 1\n",
    "        \n",
    "        if wins + .5 * ties >= 55:\n",
    "            print(\"Challenger network won {} games and tied {} games; it becomes new control network\".format(wins, ties))\n",
    "            torch.save(self.current_network.state_dict(), \"control_weights_{}.pth\".format(self.n_batches))\n",
    "            self.control_network.load_state_dict(self.current_network.state_dict())\n",
    "        else:\n",
    "            print(\"Challenger network not sufficiently better; {} wins and {} ties\".format(wins, ties))\n",
    "        \n",
    "        self.control_network.eval()\n",
    "        self.current_network.train()            \n",
    "    \n",
    "    def train_on_batch(self, batch_size = 32):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        self.current_network.train()\n",
    "        \n",
    "        sample = self.replay_buffer.sample(batch_size)\n",
    "        states, probs, rewards = zip(*sample)\n",
    "        states = torch.FloatTensor(states).requires_grad_(True)\n",
    "        probs = torch.FloatTensor(probs).requires_grad_(True)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).requires_grad_(True)\n",
    "        self.optim.zero_grad()\n",
    "        \n",
    "        ps, vs = trainer.current_network(states)\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(vs, rewards) - (ps.log() * probs).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.optim.step()\n",
    "        \n",
    "        self.n_batches += 1\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def run(self, total_runs = 100, self_play_games = 20, training_batches = 30, batch_size = 32):\n",
    "        losses = []\n",
    "        for run_num in range(1, total_runs+1):\n",
    "            print(\"Run {} of {}\".format(run_num, total_runs))\n",
    "            for selfplay_num in range(1, self_play_games + 1):\n",
    "                self.generate_self_play_data(self_play_games)\n",
    "                print(\"\\tFinished self-play game {} of {} (Buffer size {})\".format(selfplay_num, self_play_games, len(self.replay_buffer)))\n",
    "            print(\"Finished {} self-play games\".format(self_play_games))\n",
    "            for _ in range(training_batches):\n",
    "                losses.append(self.train_on_batch(batch_size))\n",
    "                if len(losses) == 5:\n",
    "                    print(\"\\tLoss for last 5 batches: {}\".format(sum(losses)))\n",
    "                    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SelfPlayTrainer(AlphaMCTSAgent(), NestedTTT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 100\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "sample = trainer.replay_buffer.sample(batch_size)\n",
    "states, probs, rewards = zip(*sample)\n",
    "states = torch.FloatTensor(states).requires_grad_(True)\n",
    "probs = torch.FloatTensor(probs).requires_grad_(True)\n",
    "rewards = torch.FloatTensor(rewards).unsqueeze(1).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, vs = trainer.current_network(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train_on_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps, vs = trainer.current_network(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.current_network.state_dict(), \"latest_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:retro]",
   "language": "python",
   "name": "conda-env-retro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
